{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "831a5a9e-0c89-4586-85f8-44ad99e8b3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%pip install --disable-pip-version-check         torch==1.13.1         torchdata==0.5.1 --quiet\\n\\n%pip install         transformers==4.27.2         datasets==2.11.0 --quiet'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%pip install --disable-pip-version-check \\\n",
    "        torch==1.13.1 \\\n",
    "        torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "        transformers==4.27.2 \\\n",
    "        datasets==2.11.0 --quiet'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ee7954-1a48-4daf-b1a8-6bc4709e3511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%pip install --upgrade pip\\n%pip install --disable-pip-version-check     torch==1.13.1     torchdata==0.5.1 --quiet\\n\\n%pip install     transformers==4.27.2     datasets==2.11.0  --quiet'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0  --quiet'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fb2855-4c27-49e6-8e2c-c200bde5bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import reddit_client_id, reddit_client_secret, reddit_user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65417753-6fe4-4074-afa0-088d7957d2ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reddit_client_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpraw\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize the Reddit API wrapper\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m reddit \u001b[38;5;241m=\u001b[39m praw\u001b[38;5;241m.\u001b[39mReddit(client_id\u001b[38;5;241m=\u001b[39m\u001b[43mreddit_client_id\u001b[49m,\n\u001b[0;32m      4\u001b[0m                      client_secret\u001b[38;5;241m=\u001b[39mreddit_client_secret,\n\u001b[0;32m      5\u001b[0m                      user_agent\u001b[38;5;241m=\u001b[39mreddit_user_agent)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Replace 'python' with the name of the subreddit you want to pull posts from\u001b[39;00m\n\u001b[0;32m      8\u001b[0m subreddit \u001b[38;5;241m=\u001b[39m reddit\u001b[38;5;241m.\u001b[39msubreddit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCryptoCurrency\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reddit_client_id' is not defined"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "# Initialize the Reddit API wrapper\n",
    "reddit = praw.Reddit(client_id=reddit_client_id,\n",
    "                     client_secret=reddit_client_secret,\n",
    "                     user_agent=reddit_user_agent)\n",
    "\n",
    "# Replace 'python' with the name of the subreddit you want to pull posts from\n",
    "subreddit = reddit.subreddit('CryptoCurrency')\n",
    "\n",
    "for submission in reddit.subreddit(\"CryptoCurrency\").hot(limit=10):\n",
    "    print(submission.title)\n",
    "\n",
    "#TODO: extract headings and top level comments for top n threads (hot n?)\n",
    "#TODO: store this where? Patch as json to some store - blob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c639634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the dataset\n",
    "submissions_dataset = []\n",
    "comments_dataset = []\n",
    "\n",
    "for submission in reddit.subreddit(\"CryptoCurrency\").hot(limit=10):\n",
    "    submissions_dataset.append([submission.title, submission.date])\n",
    "    for top_level_comment in submission.comments:\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            dataset.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4698fee",
   "metadata": {},
   "source": [
    "Note: # From qualitative analysis: a comment doesn't often have multiple coins+strong sentiment associated with each one, usually it's one coin mention and discussion, it may mention other coins but the comment itself will center around one of them and only have strong sentiment associated with that one coin. Comments like \"BTC is amazing, Etherium sucks, Doge is the best\" are rare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743148c9-5c7a-403f-b502-85a8aa18aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: what are we extracting? Coin mentions, sentiment??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0738d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BTC/Bitcoin mentions - need to map name/abbreviation to the same coin\n",
    "# Can potentially be done by the LLM\n",
    "# If not, can scrape the coinwire page and write a function to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5cb0e6-9d94-4637-8a87-09654dbe5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: load the base model using tensorflow\n",
    "\n",
    "# Loading the base model, the tokenizer and the config\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734dc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c0f3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e1f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d9b4a-087d-4363-838f-43a8da50bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write a prompt\n",
    "\n",
    "prompt = ''''''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
